# CS 231n

> Systematical learning **Machine Learning in CV**

---

## Resources

- [My Repository](https://github.com/fightingff/CS231n)
- [Official Notes](https://cs231n.github.io)
- [Course Website](http://cs231n.stanford.edu/)

---

## Some newly-learned thoughts

### Training sets, validation sets & test sets

- We should isolate the test sets with the training process, "just test the final model at the very end" to avoid overfitting
- Often validation sets are extracted from training sets

### Parameters in Linear Model

- We can view the Weight Matrix as a set of templates, and the scores as the similarity between the input and the templates
- So when printing the weights, we can somehow visualize the templates we learned
- Picture below is what we learned from the CIFAR-10 dataset using SVM
  ![template](template.png)

### SVM (Support Vector Machine)

- **Hinge Loss**: $L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + margin)$, so-called **max-margin loss** because it encourages the correct class to have a score higher than the incorrect class by at least a margin
- The hinge loss is "easy to be satisfied" since it only cares about the margin, not the exact value of the score (*e.g. [1,0] & [100,-100] both have the same loss when the margin is 1*)

### Differentiation on Vectors

- **Split**: Trying to do the differentiation on a smaller vector or even a single element instead of a matrix.

> For example, solving Softmax - Cross Entropy Loss:
> $x$ is a 1-D linear vector
> $\frac{\partial{Loss}}{\partial{x}}=\frac{\partial{Loss}}{\partial{score}} \frac{\partial{score}}{\partial{x}}= -\frac{1}{score_i} \frac{\partial{score_i}}{\partial{x_i}}=-\frac{1}{score_i} score_i (score_i - (y_i == i))$

- **Dimension**: Use Dimension to check or gain a overview of the result.

### Process

- **Preprocessing matters a lot.**

  - **Mean subtraction**: Subtract the mean of the data, thus the data should be centered around the origin
  - **Normalization**: Divide the data by the standard deviation, thus the data should be normalized to a similar scale
  - **PCA**: Reduce the dimension of the data, thus the data should be more efficient to compute.For example, SVD etc.
  - **Feature extraction**: Extract the features from the raw data, thus the data should be more informative to the model. For example, HOG, Color Histogram etc.
  - **Data Augmentation**: Generate more data from the original data, thus the model should be more robust to the noise. For example, flip, rotate, crop etc.
- **Training**

  - **Optimization**: The gradient descent, the stochastic gradient descent, the mini-batch gradient descent, the momentum, the RMSprop, the Adam etc.
  - **Hyparameter Debug**: The learning rate, the regularization strength, the number of hidden units, the number of layers, the number of epochs, the batch size etc.
  - **Monitor the Process**: The loss, the accuracy, the gradient, the weights, the features etc.
  - **Visualize the Result**: The weights, the features, the templates etc.

---

## Assignment

---

### Assignment 1

#### K-NN (K Nearest Neighbors)

- No training, just memorizing the data
- In prediction, compute the **Distance** with every sample (**Costly**)
- Use K-fold cross validation to find the best K. Concretely speaking, split the training sets into K folds, and choose each as validation set, and evaluate the model finally.

#### SVM (Support Vector Machine)

- **Hinge Loss**: $L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + margin)$
- **Regularization**: $L = \frac{1}{N} \sum_i L_i + \frac{1}{2}\lambda |W|^2$, where $\lambda$ is the regularization strength
- **Gradient Descent**:

  - $W -= \alpha \nabla_W L$,
  - $\nabla_W L_{yi} = - \Sigma_{j \neq y_i}1(w_jx_i + \Delta > 0)x_i$
  - $\nabla_W L_i = 1(w_jx_i + \Delta > 0)x_i$

#### Softmax

- **Cross Entropy Loss**: $L_i = -\log(\frac{e^{score_{y_i}}}{\Sigma_j e^{score_j}})$
- **Gradient Descent**: $W -= \alpha \nabla_W L$, where $\nabla_W L_i = -x_i(\frac{e^{score_{y_i}}}{\Sigma_j e^{score_j}} - 1)$

#### 2-Layer Network

- Combination of the lessons above. Not so hard to complete.
- *Softmax gradient, however, seems hard to do right? But the final implement of training 2-layer network runs well and achieves the accuray of about 50%*
- ![The visualized template](network.png)

#### Feature Extraction

- **Color Histogram**: Count the number of pixels in each color channel
- **HOG (Histogram of Oriented Gradients)**: Count the number of gradients in each direction
- **Training on raw pixel V/S on features**: After extracting the features, the model can outperform the raw pixel model a lot.

---

### Assignment 2

#### Fully Connected Neural Network

- Multi-layer packaged class
- **Optimization**
  - **SGD**(Stochastic Gradient Descent):

    > $W -= \alpha \ d_W$
    >

    - Simplest way (converge slowly)
  - **SGD_Momentum**:

    > $v = \beta v - \alpha \ d_W$
    >

    > $W += v$
    >

    - Simulate the physical process as ball rolling down the hill
  - **RMSprop**:

    > $cache = \beta cache + (1-\beta) d_W^2$
    >

    > $W -= \alpha \frac{d_W}{\sqrt{cache} + \epsilon}$ ($\epsilon$ avoids division by zero)
    >

    - Adjust the learning rate adaptively, but the gradient may become too small
  - **Adam**:

    > $m = \beta_1 m + (1-\beta_1) d_W$
    >

    > $v = \beta_2 v + (1-\beta_2) d_W^2$
    >

    > $mt = \frac{m}{1-\beta_1^t}$
    >

    > $vt = \frac{v}{1-\beta_2^t}$
    >

    > $W -= \alpha \frac{mt}{\sqrt{vt} + \epsilon}$
    >

    - Combine the advantages of **RMSprop** and **Momentum**

#### **Normalization**

- **Batch Normalization**:

  - Normalize the input of each layer, thus the model should be more robust to the noise and the gradient should be more stable, - But the performance depends on the batch size a lot.
  - [Gradient reference](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)
- **Layer Normalization**:

  - Normalize the input of each sample, thus the model should be more robust to the noise and the gradient should be more stable.
  - But the performance may be influenced by the feature dimensions.
  - Gradient is easy. We can transpose the matrix and transform the problem into the batch normalization.
  - **(N,D) -> (D,N)**
- *Problem: The result of my code seems somewhat strange, like normalization works worse than without normalization. I think the problem may be the learning rate or the batch size, but I have no energy to debug it.*
  ![batch](normalization1.png)
  ![layer](normalization2.png)

#### **Dropout**

- Randomly set some neurons to zero, thus the model should be more robust to the noise and the overfitting should be avoided.
- `mask = (np.random.rand(*x.shape) < p) / p`
- `p` is the probability of keeping a neuron active, and thus the mean of the output (or mathematical expectation) should be the same as the input.

#### **Convolutional Neural Network**

- **Convolution Layer**

  - Filter

    - The learned template to extract the features
    - Often $3*3*C$ with stride 1
    - **Share parameters**. Using the same filter to do convolution on the whole input, thus the model should be more efficient to compute and the number of parameters should be highly reduced.
    - *Too large filter may lead to a linear model, and costs more memory.*
    - Examples of features

    ![cnn_feature](cnn_feature.png)
  - Padding

    - Add some zeros around the input, thus the output should be the same size as the input, which is convenient for the next layer to compute.
    - Avoid the information loss on edges.
    - $P = \frac{F-1}{2}$, where $F$ is the size of the filter
  - Pooling

    - Max Pooling: $max(x)$ (Popular in practice, often $2 * 2$)
    - Average Pooling: $mean(x)$
    - Reduce the size of the input, thus the model should be more efficient to compute and the number of parameters should be highly reduced.
    - Avoid the overfitting and the noise.
  - **Layer Strutcture**

    - Input: $N * C * H * W$
    - Filter: $F * C * HH * WW$
    - Output: $N * F * H' * W'$
    - Activation (often Relu): $N * F * H' * W'$
    - Pooling: $N * F * H'' * W''$
- **Normalization**

  - Spatial Batch Normalization

    - Similar to Batch Normalization.
    - `X = x.transpose(0, 2, 3, 1).reshape(-1, C)`
  - Group Normalization

    - Similar to Layer Normalization.
    - `X = x.reshape(N*G, -1)`

#### **PyTorch**

- Useful tool to build the model and train the model
- [Tutorial](https://pytorch.org/tutorials/)
- [Docs](https://pytorch.org/docs/stable/index.html)
- **Structure**

  - `nn.Module`

    - Personalized model, more flexible
    - Implement `forward` function
  - `nn.Sequential`

    - Simple model, just concatenate the layers
- **Optimization**

  - `optim.SGD`
  - `optim.Adam`
  - `optim.RMSprop`
  - Backward Propagation
  
  ```python

      optimizer.zero_grad() # clear

      loss.backward()       # backpropagation
      
      optimizer.step()      # update
  ```
  >  optimizer.zero_grad() # clear
  >
  >  loss.backward()       # backpropagation
  >
  >  optimizer.step()      # update

- **Training**
